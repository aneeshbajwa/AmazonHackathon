{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7213f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b3187a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding data from 0 to 49841 for client : client_1\n",
      "Adding data from 49841 to 99682 for client : client_2\n",
      "Adding data from 99682 to 149523 for client : client_3\n",
      "Adding data from 149523 to 199364 for client : client_4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import numpy as np\n",
    "from tensorflow import reshape, nest, config\n",
    "from tensorflow.keras import losses, metrics, optimizers\n",
    "import tensorflow_federated as tff\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "split = 4\n",
    "NUM_ROUNDS = 5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 1\n",
    "PREFETCH_BUFFER = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('../Downloads/creditcard.csv')\n",
    "x = np.asanyarray(df.drop('Class',1))\n",
    "y = np.asanyarray(df['Class'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.7)\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE()\n",
    "\n",
    "X_train,y_train = sm.fit_resample(x_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train = x_train.astype(np.float32)/x_train.max()\n",
    "y_train = y_train.astype(np.int32)/y_train.max()\n",
    "x_test = x_test.astype(np.float32)/x_test.max()\n",
    "y_test = y_test.astype(np.int32)/y_test.max()\n",
    "n_input = x_train.shape[1]\n",
    "total_count = len(x_train)\n",
    "data_per_set = int(np.floor(total_count/split))\n",
    "\n",
    "client_train_dataset = collections.OrderedDict()\n",
    "for i in range(1, split+1):\n",
    "    client_name = \"client_\" + str(i)\n",
    "    start = data_per_set * (i-1)\n",
    "    end = data_per_set * i\n",
    "\n",
    "    print(f\"Adding data from {start} to {end} for client : {client_name}\")\n",
    "    data = collections.OrderedDict((('label', y_train[start:end]), ('features', x_train[start:end])))\n",
    "    client_train_dataset[client_name] = data\n",
    "\n",
    "\n",
    "SHUFFLE_BUFFER = data_per_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8042ac8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=OrderedDict([('label', TensorSpec(shape=(), dtype=tf.float64, name=None)), ('features', TensorSpec(shape=(30,), dtype=tf.float32, name=None))])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tff.simulation.datasets.TestClientData(client_train_dataset)\n",
    "sample_dataset = train_dataset.create_tf_dataset_for_client(train_dataset.client_ids[0])\n",
    "sample_element = next(iter(sample_dataset))\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eeb251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "\n",
    "  def batch_format_fn(element):\n",
    "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
    "\n",
    "    return collections.OrderedDict(\n",
    "        x=reshape(element['features'], [-1,1,30]),\n",
    "        y=reshape(element['label'], [-1, 1]))\n",
    "\n",
    "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
    "      BATCH_SIZE).map(batch_format_fn).prefetch(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31b0cdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of client datasets: 4\n",
      "First dataset: <PrefetchDataset element_spec=OrderedDict([('x', TensorSpec(shape=(None, 1, 30), dtype=tf.float32, name=None)), ('y', TensorSpec(shape=(None, 1), dtype=tf.float64, name=None))])>\n"
     ]
    }
   ],
   "source": [
    "preprocessed_sample_dataset = preprocess(sample_dataset)\n",
    "\n",
    "\n",
    "def make_federated_data(client_data, client_ids):\n",
    "    return [preprocess(client_data.create_tf_dataset_for_client(x)) for x in client_ids]\n",
    "\n",
    "federated_train_data = make_federated_data(train_dataset, train_dataset.client_ids)\n",
    "\n",
    "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
    "print('First dataset: {d}'.format(d=federated_train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c64a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee1acf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( -> <global_model_weights=<trainable=<float32[30,65],float32[65],float32[65,1],float32[1]>,non_trainable=<>>,distributor=<>,client_work=<>,aggregator=<value_sum_process=<>,weight_sum_process=<>>,finalizer=<int64,float32[30,65],float32[65],float32[65,1],float32[1],float32[30,65],float32[65],float32[65,1],float32[1]>>@SERVER)\n"
     ]
    }
   ],
   "source": [
    "def create_keras_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(65, input_shape=(1,30), kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_fn():\n",
    "    keras_model = create_keras_model()\n",
    "    return tff.learning.from_keras_model(\n",
    "          keras_model,\n",
    "          loss=losses.BinaryCrossentropy(),\n",
    "          input_spec=preprocessed_sample_dataset.element_spec,\n",
    "           \n",
    "          metrics=[tf.keras.metrics.Accuracy]\n",
    "      )\n",
    "\n",
    "\n",
    "\n",
    "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.02),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=1.0))\n",
    "print(str(iterative_process.initialize.type_signature))\n",
    "\n",
    "state = iterative_process.initialize()\n",
    "\n",
    "tff_train_acc = []\n",
    "tff_val_acc = []\n",
    "tff_train_loss = []\n",
    "tff_val_loss = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c892cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.0), ('loss', 0.02522428), ('num_examples', 996820), ('num_batches', 996820)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
     ]
    }
   ],
   "source": [
    "result = iterative_process.next(state, federated_train_data)\n",
    "state = result.state\n",
    "metrics = result.metrics\n",
    "print('round  1, metrics={}'.format(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "076a77aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.0), ('loss', 0.025300208), ('num_examples', 996820), ('num_batches', 996820)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n",
      "round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.96241546), ('loss', 0.025300207), ('num_examples', 996820), ('num_batches', 996820)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n",
      "round  4, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('accuracy', 0.9983598), ('loss', 0.025300207), ('num_examples', 996820), ('num_batches', 996820)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
     ]
    }
   ],
   "source": [
    "NUM_ROUNDS = 5\n",
    "for round_num in range(2, NUM_ROUNDS):\n",
    "  result = iterative_process.next(state, federated_train_data)\n",
    "  state = result.state\n",
    "  metrics = result.metrics\n",
    "  print('round {:2d}, metrics={}'.format(round_num, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7d6dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"/tmp/logs/scalars/training/\"\n",
    "summary_writer = tf.summary.create_file_writer(logdir)\n",
    "state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fec6c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with summary_writer.as_default():\n",
    "  for round_num in range(1, NUM_ROUNDS):\n",
    "    result = iterative_process.next(state, federated_train_data)\n",
    "    state = result.state\n",
    "    metrics = result.metrics\n",
    "    for name, value in metrics['client_work']['train'].items():\n",
    "      tf.summary.scalar(name, value, step=round_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdf80524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events.out.tfevents.1658934507.ANEESHs-MacBook-Air.local.32230.0.v2\r\n",
      "events.out.tfevents.1658934654.ANEESHs-MacBook-Air.local.34508.0.v2\r\n",
      "events.out.tfevents.1658940218.ANEESHs-MacBook-Air.local.35602.0.v2\r\n",
      "events.out.tfevents.1659024913.ANEESHs-MacBook-Air.local.36751.0.v2\r\n",
      "events.out.tfevents.1659246725.ANEESHs-MacBook-Air.local.51605.0.v2\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "!ls {logdir}\n",
    "%tensorboard --logdir {logdir} --port=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deebe1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b21343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c6a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
